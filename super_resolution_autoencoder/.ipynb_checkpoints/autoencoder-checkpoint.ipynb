{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\"\"\"\nImage Super-Resolution using Convolutional\nAutoencoders\nAuthor: Amruth Karun M V\nDate: 06-Nov-2021\n\"\"\"\n\nimport cv2\nimport numpy as np\nimport random\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras import Model, Input, regularizers\nfrom tensorflow.keras.layers import (\n    Dense, Conv2D, MaxPool2D, \n    UpSampling2D, Add)\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom keras.preprocessing import image\nimport glob\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport warnings;\nwarnings.filterwarnings('ignore')\n\n\nINPUT_PATH = '../input/cifar10/cifar10_sample/'\n\ndef show_sample_image():\n    \"\"\"\n    Displays original 32 x 32 images.\n    Arguments: None\n    Returns: Displays the image\n    \"\"\"\n    \n    cifar_sample = glob.glob(INPUT_PATH + '*.png')\n    random_index = random.randint(0, 19)\n    print(\"Image: \", random_index)\n    img_path = cifar_sample[random_index]\n    img = cv2.imread(img_path)\n    plt.imshow(img)\n        \ndef load_images():\n    \"\"\"\n    Loads sample images from cifar10 dataset.\n    2 images are taken from each class. Total 20 images.\n    Arguments: None\n    Returns: Train and val images\n    \"\"\"\n    \n    cifar_sample = glob.glob(INPUT_PATH + '*.png')\n    print(\"Total images = \", len(cifar_sample))\n    all_images = []\n    for i in tqdm(cifar_sample):\n        img = image.load_img(i, target_size=(32,32,3))\n        img = image.img_to_array(img)\n        img = img/255.\n        all_images.append(img)\n    all_images = np.array(all_images)\n    train_x, val_x = train_test_split(all_images, random_state=32, test_size=0.2)\n    return train_x, val_x\n\ndef pixalate_image(image, scale_percent = 50):\n    \"\"\"\n    Lower the resolution of input image without\n    reducing the size\n    Arguments:\n        image         -- input image\n        scale_percent -- amount to be reduced\n    Returns: Pixalated image\n    \"\"\"\n    \n    width = int(image.shape[1] * scale_percent / 100)\n    height = int(image.shape[0] * scale_percent / 100)\n    dim = (width, height)\n    small_image = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n    width = int(small_image.shape[1] * 100 / scale_percent)\n    height = int(small_image.shape[0] * 100 / scale_percent)\n    dim = (width, height)\n    low_res_image = cv2.resize(small_image, dim, interpolation = cv2.INTER_AREA)\n    return low_res_image\n\n\ndef get_low_res_image(train_x, val_x):\n    \"\"\"\n    Get low resolution images for train\n    and validation set\n    Arguments:\n        train_x  -- train set\n        val_x    -- validation set\n    Returns: Low resolution data\n    \"\"\"\n    \n    # get low resolution images for the train set\n    train_x_px = []\n  \n    for i in range(train_x.shape[0]):\n        temp = pixalate_image(train_x[i,:,:,:])\n        train_x_px.append(temp)\n    train_x_px = np.array(train_x_px)   \n    \n    # get low resolution images for the validation set\n    val_x_px = []\n    for i in range(val_x.shape[0]):\n        temp = pixalate_image(val_x[i,:,:,:])\n        val_x_px.append(temp)\n    val_x_px = np.array(val_x_px)     \n    \n    return train_x_px, val_x_px\n\ndef train_model(train_x_px, val_x_px):\n    \"\"\"\n    Trains the  Autoencoder network\n    Arguments: \n        train_x_px    -- low resolution train set\n        val_x_px      -- low resolution validation set\n    Returns: Autoencoder Network\n    \"\"\"\n    \n    Input_img = Input(shape=(32, 32, 3))  \n    #encoding architecture\n    x1 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(Input_img)\n    x2 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x1)\n    x3 = MaxPool2D(padding='same')(x2)\n    x4 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x3)\n    x5 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x4)\n    x6 = MaxPool2D(padding='same')(x5)\n    encoded = Conv2D(256, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x6)\n    \n    # decoding architecture\n    x7 = UpSampling2D()(encoded)\n    x8 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x7)\n    x9 = Conv2D(128, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x8)\n    x10 = Add()([x5, x9])\n    x11 = UpSampling2D()(x10)\n    x12 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x11)\n    x13 = Conv2D(64, (3, 3), activation='relu', padding='same', kernel_regularizer=regularizers.l1(10e-10))(x12)\n    x14 = Add()([x2, x13])\n    decoded = Conv2D(3, (3, 3), padding='same',activation='relu', kernel_regularizer=regularizers.l1(10e-10))(x14)\n    autoencoder = Model(Input_img, decoded)\n    autoencoder.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n\n    autoencoder.summary()\n    \n    early_stopper = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=8, verbose=1, mode='auto')\n    history = autoencoder.fit(train_x_px, train_x,\n                              epochs=256,\n                              batch_size=64,\n                              shuffle=True,\n                              validation_data=(val_x_px, val_x),\n                              callbacks=[early_stopper])\n    \n    autoencoder.save_weights(\"autoencoder.h5\")\n    return autoencoder\n\ndef get_results(autoencoder, val_x, val_x_px):\n    \"\"\"\n    Evaluate the autoencoder model using\n    validation data and predict results\n    Arguments:\n        autoencoder    -- trained autoencoder model\n        val_x          -- original validation data\n        val_x_px       -- low resolution validation data\n    \"\"\"\n    \n    results = autoencoder.evaluate(val_x_px, val_x)\n    print('val_loss = {}, val_accuracy = {}'.format(results[0], results[1]))\n    \n    predictions = autoencoder.predict(val_x_px)\n    \n    n = 4\n    plt.figure(figsize= (20,10))\n    for i in range(n):\n        ax1 = plt.subplot(3, n, i+1)\n        plt.imshow(val_x_px[i])\n        ax1.get_xaxis().set_visible(False)\n        ax1.get_yaxis().set_visible(False)\n        ax1.title.set_text('Pixelated Image')\n        \n        ax2 = plt.subplot(3, n, i+1+n)\n        plt.imshow(predictions[i])\n        ax2.get_xaxis().set_visible(False)\n        ax2.get_yaxis().set_visible(False)\n        ax2.title.set_text('Predicted Image')\n        \n    plt.show()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-11-06T14:46:53.612309Z","iopub.execute_input":"2021-11-06T14:46:53.612579Z","iopub.status.idle":"2021-11-06T14:46:53.648689Z","shell.execute_reply.started":"2021-11-06T14:46:53.612549Z","shell.execute_reply":"2021-11-06T14:46:53.648012Z"},"trusted":true},"execution_count":133,"outputs":[]},{"cell_type":"code","source":"# Show 1 random image from train set\n# show_sample_image()\ntrain_x, val_x = load_images()\ntrain_x_px, val_x_px = get_low_res_image(train_x, val_x) \n\n# Train the model\nautoencoder = train_model(train_x_px, val_x_px)\nget_results(autoencoder, val_x, val_x_px)","metadata":{"execution":{"iopub.status.busy":"2021-11-06T14:46:53.649844Z","iopub.execute_input":"2021-11-06T14:46:53.650374Z","iopub.status.idle":"2021-11-06T14:47:28.223388Z","shell.execute_reply.started":"2021-11-06T14:46:53.650342Z","shell.execute_reply":"2021-11-06T14:47:28.222715Z"},"trusted":true},"execution_count":134,"outputs":[]}]}